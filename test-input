Jaime Sevilla∗ University of Aberdeen
Tamay Besiroglu
Lennart Heim
Centre for the Governance of AI
Marius Hobbhahn
Anson Ho
University of St Andrews
Pablo Villalobos
Complutense University of Madrid
COMPUTE TRENDS ACROSS THREE ERAS OF MACHINE LEARNING
Massachusetts Institute of Technology University of Tübingen
ABSTRACT
Compute, data, and algorithmic advances are the three fundamental factors that guide the progress of modern Machine Learning (ML). In this paper we study trends in the most readily quantified factor – compute. We show that before 2010 training compute grew in line with Moore’s law, doubling roughly every 20 months. Since the advent of Deep Learning in the early 2010s, the scaling of training compute has accelerated, doubling approximately every 6 months. In late 2015, a new trend emerged as firms developed large-scale ML models with 10 to 100-fold larger requirements in training compute. Based on these observations we split the history of compute in ML into three eras: the
, the and the . Overall, our work highlights the fast-growing compute requirements for training advanced ML systems.
1 Introduction
Predicting progress in the field of Machine Learning (ML) is hard but of significant relevance for actors in industry, policy, and society. How much better will Computer Vision be in a decade? Will machines ever write better fiction than us? What jobs will we be able to automate?
Answering these questions is hard because they depend on many factors. However, one factor that influences all of them has been astonishingly regular over time—compute.
Various researchers have highlighted the relationship between AI capabilities and the scaling of ML models (Kaplan et al., 2020; Sutton, 2019; Z. Li et al., 2020; Jones, 2021; Rosenfeld et al., 2019; Hestness et al., 2017). Therefore, compute can be seen as a quantifiable proxy for the progress of ML research.
This paper is a detailed investigation into the compute demand of milestone ML models over time. We make the following contributions:
1. We curate a dataset of 123 milestone Machine Learning systems, annotated with the compute it took to train them.
2. We tentatively frame the trends in compute in terms of three distinct eras: the , the and the . We offer estimates of the doubling times during each of these
eras.
3. We extensively check our results in a series of appendices, discussing alternate interpretations of the data, and
differences with previous work.
Our dataset, figures, and an interactive visualization are publicly available.2
∗Correspondence to j.sevilla.20@abdn.ac.uk
2If you use our dataset, please cite us as: Parameter, Compute and Data Trends in Machine Learning by Jaime Sevilla, Pablo Villalobos, Juan Felipe Cerón, Matthew Burtell, Lennart Heim, Amogh B. Nanjajjar, Anson Ho, Tamay Besiroglu and Marius Hobbhahn; 2021.
Pre Deep Learning Era Deep Learning Era Large-Scale Era
arXiv:2202.05924v2 [cs.LG] 9 Mar 2022
Pre Deep Learning Era
Deep Learning Era Large-Scale Era
2 Related work
Amodei & Hernandez (2018) introduced two methods for estimating training compute in AI and Compute, and analyzed a trend based on 15 ML systems. They found that scaling in ML training compute followed a 3.4 month doubling time between 2012 and 2018.
In a later addendum, Sastry et al. (2019) supplemented their analysis with 10 papers from the pre-2012 era. They found a doubling rate in training compute of about 2 years between 1959 and 2012.
Lyzhov (2021) expanded upon Amodei & Hernandez’s dataset with seven subsequently released ML models and argued that growth stalled after the publication of AI and Compute (Amodei & Hernandez, 2018). In particular, the author found that the most compute-intensive model of 2020 (GPT-3) only required 1.5× more compute for training than the most compute-intensive model of 2017 (AlphaGo Zero).
Article
Amodei & Hernandez (2018) Sastry et al. (2019)
Lyzhov (2021)
Summary of findings
∼3.4 month doubling time between 2012 and 2018 ∼2 year doubling period between 1959 and 2018 >2 year doubling period between 2018 and 2020
 
Table 1: Summary of results from previous investigations into compute trends in ML.
In a similar effort, Sevilla et al. (2021) investigated trends in trainable parameter counts. They found an 18 to 24 month doubling time in all application domains from 2000 to 2021. For language models, they found that a discontinuity occurred between 2016 and 2018, where the doubling time for parameters sped up to 4 to 8 months.
Thompson et al. (2020) studied the increasing reliance of Deep Learning on computational power. They concluded that progress was becoming increasingly infeasible as compute requirements grew faster than progress in computing hardware.
In a recent report, Lohn & Musser (2022) investigated the limits of the compute trend by extrapolating the training costs into the future (based on a 3.4 month doubling time from Amodei & Hernandez (2018)) and exploring potential limitations. The authors concluded that the current rate of increase is unsustainable due to cost, hardware availability, and engineering difficulties, and that a slow-down may have already begun.
There have been other initiatives to collect data on important ML models. Akronomicon is a publicly available leaderboard of large-scale ML models (Akronomicon, 2022). Computer Progress (2022) has been collecting information on model performance and training compute of ML models on some common benchmarks. AI Tracker (2022) is collecting information on the capabilities of modern ML models. We are collaborating with each of these three projects and (with their permission) we incorporate part of their work in our dataset.
Furthermore, Desislavov et al. (2021) investigate inference compute in Computer Vision and Natural Language Processing systems. We use their results to inform some of our estimates.
Compared to prior work, our data collection is more comprehensive. Our dataset contains three times more ML models than previous ones and includes data up to 2022. We also offer novel interpretations of previous data, which we believe have important implications for understanding progress in ML.
2
3 Trends
We explain the data we curated in terms of three distinct eras and three distinct trends. In short, there was an era of slow growth before Deep Learning took off. Around 2010, the trend sped up and has not slowed down since then. Separately, in 2015 to 2016 a new trend of large-scale models emerged, growing at a similar rate, but exceeding the previous one by two orders of magnitude (OOMs hereafter). See Figure 1 and Table 2 for a summary.
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 121
1e+24
ar E gn 1e+22
inrae 1e+20
) Lp sP eD 1e+18
Oer LF P 1e+16
( etu 1e+14
p m 1e+12
oc g 1e+10
nini
1e+8
arT arEg ar
1e+6
ni Ee
nralac
1e+4
eL S-e
pegra
1e+2
DL
1952 1960 1968
1976 1984 1992 2000 2008 2016
Publication date
                      
Figure 1: Trends in n = 121 milestone ML models between 1952 and 2022. We distinguish three eras. Notice the change of slope circa 2010, matching the advent of Deep Learning; and the emergence of a new large-scale trend in late 2015.
Period
1952 to 2010
Pre Deep Learning Trend 2010 to 2022
Deep Learning Trend September 2015 to 2022 Large-Scale Trend
Data
All models
(n = 19) Regular-scale models (n = 72) Large-scale models (n = 16)
Scale (start to end)
3e+04 to 2e+14 FLOPs 7e+14 to 2e+18 FLOPs 4e+21 to 8e+23 FLOPs
Slope
0.2 OOMs/year [0.1; 0.2; 0.2] 0.6 OOMs/year [0.4; 0.7; 0.9] 0.4 OOMs/year [0.2; 0.4; 0.5]
Doubling time
21.3 months [17.0; 21.2; 29.3] 5.7 months [4.3; 5.6; 9.0] 9.9 months [7.7; 10.1; 17.1]
             
Table 2: Summary of our main results. In 2010 the trend accelerated along the with the popularity of Deep Learning, and in late 2015 a new trend of large-scale models emerged.
First we will discuss the transition to Deep Learning circa 2010-2012. Then we will discuss the emergence of large-scale models circa 2015-2016.
We performed some alternative analyses to examine our conclusions from additional perspectives. In Appendix B we discuss trends in record-setting models. In Appendix C we discuss trends in different ML domains.
3
3.1 The transition to Deep Learning
Consistent with the results from Amodei & Hernandez (2018), we find two very different trend regimes before and after the advent of Deep Learning. Before then, the amount of compute required to train ML systems doubled once every 17 to 29 months. Subsequently, the overall trend speeds up and doubles every 4 to 9 months.
The trend in the roughly matches Moore’s law, according to which transistor density doubles roughly every two years (Moore, 1965) – often simplified to computational performance doubling every two years.
It is not clear when the starts3 — there are no noticeable discontinuities in the transition from the Pre Deep Learning to the Deep Learning era. Moreover, our results barely change if we place the start of the Deep Learning era in 2010 or in 2012, see Table 3.
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 121
1e+24 1e+22 1e+20 1e+18 1e+16 1e+14 1e+12 1e+10
1e+8 1e+6 1e+4 1e+2
1952 1960 1968
1976 1984 1992 2000
Publication date
2008 2016
Figure 2: Trends in training compute of n = 121 milestone ML systems between 1952 and 2022. Notice the change of slope in the trends circa 2010.
Period
Outliers
Scale (FLOPs)
Slope
Doubling time R2
0.77 0.83 0.70 0.78 0.58 0.69
Table 3: Log-linear regression results for ML models from 1952 to 2022.
3We discuss the start of the Deep Learning Era in more detail in Appendix D. 4
Pre Deep Learning Era
Deep Learning Era
             
Pre Deep Learning Era
  
Deep Learning Era

Training compute (FLOPs)
1952-2009 1952-2011
All models (n = 19) 3e+04 / 2e+14
0.2 OOMs/year [0.1; 0.2; 0.2] 21.3 months [16.2; 21.3; 31.3]
All models (n = 26) 1e+04 / 3e+15
0.2 OOMs/year [0.1; 0.2; 0.2] 19.6 months [15.6; 19.4; 25.0]
2010-2022 2012-2022
All models (n = 98) 1e+15 / 6e+22
0.7 OOMs/year [0.6; 0.7; 0.7] 5.6 months [5.0; 5.6; 6.2]
Regular-scale (n = 77) 4e+14 / 2e+22
0.7 OOMs/year [0.6; 0.7; 0.7] 5.6 months [5.1; 5.6; 6.2]
All models (n = 91) 1e+17 / 6e+22
0.6 OOMs/year [0.5; 0.6; 0.7] 5.7 months [4.9; 5.7; 6.7]
Regular-scale (n = 72) 4e+16 / 2e+22
0.6 OOMs/year [0.5; 0.6; 0.7] 5.7 months [4.9; 5.7; 6.7]

3.2 Trends in the Large-Scale era
Our data suggests that around 2015-2016 a new trend of emerged, see Figure 3. This new trend began with AlphaGo in late 2015 and continues up to the present day. These large-scale models were trained by large corporations, whose larger training budgets presumably enabled them to break the previous trend.
Note that we made an intuitive decision in deciding which systems belong to this new large-scale trend. We justified it post hoc as the systems that exceed a certain Z-value threshold with respect to nearby models, see Appendix A for details on our method. See Appendix F for discussion on what makes large-scale models categorically different. There is room for alternative interpretations of the data.
Separately, the continued unperturbed. This trend before and after 2016 is continuous and has the same slope, doubling every 5 to 6 months, see Table 4.4
The trend of increasing compute in is apparently slower, doubling every 9 to 10 months. Since we
have limited data on these models, the apparent slow-down might be the result of noise.5
Our results contrast with Amodei & Hernandez (2018), who find a much faster doubling period of 3.4 months between 2012 and 2018, and with Lyzhov (2021), who finds a much longer doubling period of >2 years between 2018 and 2020. We make sense of these discrepancies by noting that their analyses have limited data samples and assume a single trend 6, while ours studies large-scale and regular-scale models separately. Since the large-scale trend only recently emerged, previous analyses could not differentiate these two distinct trends.7
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 102
1e+25 1e+24 1e+23 1e+22 1e+21 1e+20 1e+19 1e+18 1e+17 1e+16 1e+15 1e+14
2011 2012
2013 2014
2015 2016 2017
Publication date
2018 2019 2020
2021 2022
Figure 3: Trends in training compute of n102 milestone ML systems between 2010 and 2022. Notice the emergence of a possible new trend of large-scale models around 2016. The trend in the remaining models stays the same before and after 2016.
4Among other reasons, this reinforces our belief that the trend of large-scale models is a separate one.
5In Appendix G we discuss some possible causes for this potential slowdown. In Appendix B we also show that the trend is equally fast before and after September 2015 if we look only at record-setting models.
7We discuss this in more depth in Appendix E.
7Arguably we should pay most attention to the most compute-intensive models overall – these are the ones most likely to advance the frontier. We do so in Appendix B, where we look at trends in record-setting models and find results consistent with those presented in this section.
5
large-scale models
trend of regular-scale models
large-scale models
          
Deep Learning Era
Large-Scale Era
KN5 LM + RNN 400/10 (WSJ)
Word2Vec (large)
R-FCN
RNN 500/10 + RT09 LM (NMISCTDRNTN0(5M) NIST) DQN
Feedforward NN 6-layer MLP (MNIST)
VGG16
AlphaX-1
Seq2Seq SPPNet
DLRM-2020 Decoupled weight deCcraoyssr-elignugluaarilzaltiigonment
RNNsearch-50G*oogLeNet / InceptionV1 AlexNet VisualizingGCANNs
Dropout (MNIST)
TransE
Mitosis Part-of-sentence tagging model Named Entity Recognition model
AlphaGo Master
GNMT
AlphaStar Meena
OpenAI Five Megatron-BERT
Meta Pseudo LabelAsLIGN
SPwroittcTh5-XXL HPyapneGrCul-oαva
NASv3 (CIFAR-10)
GPT-2
MnasNet-AM1n+aSsNSeDtL-Aite3 OnceforAllCPM-Large
AlphaGoLee AlphaGo Fan
Libratus
NEO (DL:RM-2022)
OpenAI TI7 DOTA 1v1 JFT
Rubik's cube
wave2vec 2.0 LARGE DLRM-2P0r2o1Gen
MoE MSRA (C, PReLDUe)epSpeech2
ResNet-152 (ImageNet) GPT Transformer
AlphaFold
KEPLER
Xception
PNASNet-5 YOLOv3
AlphaGo Zero
AlphaZero
Megatron-Turing NLG 530B Gopher
GPT-3 175B Jurassic-1-JumYubaonL1a.M0 DA
BigGAN-deep 512x512
ViT-G/14 ALBERT-xxlarge AraGPT2-Mega
AmoebaNet-A IMPALA
BERT-Large
Population-based DRPLroxylessNAS ObjectNet
T5-11B
iGPT-XL
DALL-E GShard (dense) CogView
Turing NLG GPT-J-6B OpenAIGFSivheard (60V0iBT-)H/14
MegatronT-L5M-3BCLIP (iVGiTPTL-/L14@33G6PpTx-)Neo
Transformer local-attention (NesT-B)
Primer HuBEMR6T-10T SEER
 
Training compute (FLOPs)
R2
0.55 0.33 0.48 0.66
Table 4: Results of a log-linear regression for data between 2010 and 2022. The trend of regular-scale models before 2015 continues uninterrupted afterwards.
4 Conclusion
In this article, we have studied trends in compute by curating a dataset of training compute with more than 100 milestone ML systems and used this data to analyze how the trend has grown over time.
Our findings seem consistent with previous work, though they indicate a more moderate scaling of training compute. In particular, we identify an 18-month doubling time between 1952 and 2010, a 6-month doubling time between 2010 and 2022, and a new trend of large-scale models between late 2015 and 2022, which started 2 to 3 orders of magnitude over the previous trend and displays a 10-month doubling time.
To summarize: in the compute grew slowly. Around 2010, the trend accelerated as we transitioned into the . In late 2015, companies started releasing large-scale models that surpassed
the trend, e.g. AlphaGo – marking the beginning of the Large-Scale Era . Framing the trends in terms of these three eras helps us explain the discontinuities we observed in the data, though we are not confident in the distinction between large-scale and regular-scale models.
We hope our work will help others better understand how much recent progress in ML has been driven by increases in scale and improve our forecasts for the development of advanced ML systems.
Moreover, the growing trend in training compute highlights the strategic importance of hardware infrastructure and engineers. Cutting-edge research in ML has become synonymous with access to large compute budgets or computing clusters, and expertise to leverage them.
One aspect we have not covered in this article is another key quantifiable resource used to train Machine Learning models — data. We will be looking at trends in dataset size and their relationship to trends in compute in future work.
Acknowledgments
We thank Alex Lyzhov, Girish Sastry, Danny Hernandez, Haydn Belfield, Jack Clark, Markus Anderljung, Alexis Carlier, Noemi Dreksler, Ben Garfinkel, Anton Korinek, Toby Shevlane, Stella Biderman, and Robert Trager. Jaime Sevilla is funded by the Open Philanthropy Project. Lennart Heim conducted part of this work during a summer fellowship in 2021 at the Stanford Existential Risks Initiative (SERI).
References
Adiwardana, D., & Luong, T. (2020). Towards a Conversational Agent that Can Chat About. . . Anything. Retrieved 2022- 02-23, from http://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html
AI Tracker. (2022). AI Tracker. Retrieved from https://www.aitracker.org/
Ajmera, A., & Ramakrishnan, M. (2021, June). Ford to Shut Some N. American Plants for Few Weeks on Chip Shortage.
(Published: Reuters)
The Akronomicon — LightOn AI Research. (2022). Akronomicon. Retrieved from https://lair.lighton.ai/
 akronomicon/

Alom, M. Z., Taha, T. M., Yakopcic, C., Westberg, S., Sidike, P., Nasrin, M. S., . . . Asari, V. K. (2018). The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches. (_eprint: 1803.01164)
Alvi, A., & Kharya, P. (2021, October). Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model. Retrieved 2022-02-23, from https:// www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron
-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/

Period Data Scale (FLOP) Slope Doubling time
2010-2016 All models (n = 20) 6e+14 / 3e+18 0.7 OOMs/year [0.4; 0.6; 0.9] 5.3 months [3.9; 5.2; 8.5]
All models (n = 79) 1e+19 / 5e+22 0.5 OOMs/year [0.4; 0.6; 0.8] 6.7 months [4.9; 6.6; 10.0]
2016-2022 Regular-scale (n = 60) 3e+18 / 2e+22 0.6 OOMs/year [0.5; 0.6; 0.8] 5.9 months [4.4; 5.8; 7.9]
Large-scale (n = 19) 4e+21 / 6e+23 0.3 OOMs/year [0.1; 0.3; 0.5] 10.7 months [7.9; 10.6; 25.6]
Pre Deep Learning Era
Deep Learning Era
6
Amodei, D., Anubhai, R., Battenberg, E., Case, C., Casper, J., Catanzaro, B., . . . Zhu, Z. (2015, December). Deep Speech 2: End-to-End Speech Recognition in English and Mandarin. arXiv:1512.02595 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1512.02595
Amodei, D., & Hernandez, D. (2018, May). Review of AI and Compute. (Published: OpenAI Blog)
Antoun, W., Baly, F., & Hajj, H. (2020, December). AraGPT2: Pre-Trained Transformer for Arabic Language
Generation. Retrieved 2022-02-23, from https://arxiv.org/abs/2012.15520v1
Athlur, S., Saran, N., Sivathanu, M., Ramjee, R., & Kwatra, N. (2021). Varuna: Scalable, Low-cost Training of Massive
Deep Learning Models. (_eprint: 2111.04007)
Attinasi, M. G., Stefani, R. D., Frohm, E., Gunnella, V., Koester, G., Tóth, M., & Melemenidis, A. (2021, June). The Semiconductor Shortage and Its Implication for Euro Area Trade, Production and Prices. (Published: ECB Economic Bulletin)
Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016, July). Layer Normalization. Retrieved 2022-02-23, from https:// arxiv.org/abs/1607.06450v1
Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020, June). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. Retrieved 2022-02-23, from https://arxiv.org/abs/2006.11477v3
Bahdanau, D., Cho, K., & Bengio, Y. (2016, May). Neural Machine Translation by Jointly Learning to Align and Translate. arXiv:1409.0473 [cs, stat]. Retrieved 2022-02-23, from http://arxiv.org/abs/1409.0473
Baidu Research. (n.d.). Retrieved 2022-02-23, from http://research.baidu.com/Blog/index-view?id=160
Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., & Mordatch, I. (2019, September). Emergent Tool Use from Multi-Agent Interaction. Retrieved 2022-02-23, from https://openai.com/blog/ emergent-tool-use/
Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., . . . Katz, B. (2019). ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In Advances in Neural Information Processing Systems (Vol. 32). Curran Associates, Inc. Retrieved 2022-02-23, from https://papers.nips.cc/ paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html
Bard, N., Foerster, J. N., Chandar, S., Burch, N., Lanctot, M., Song, H. F., . . . Bowling, M. (2019, February). The Hanabi Challenge: A New Frontier for AI Research. Retrieved 2022-02-23, from https://arxiv.org/abs/ 1902.00506v2 doi: 10.1016/j.artint.2019.103216
Barrett, E. (2021, June). Taiwan’s drought is exposing just how much water chipmakers like TSMC use (and reuse). Retrieved from https://fortune.com/2021/06/12/chip-shortage-taiwan-drought-tsmc-water
-usage/
Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003, March). A neural probabilistic language model. The Journal of Machine Learning Research, 3(null), 1137–1155.
Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., & Yakhnenko, O. (2013). Translating Em- beddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems (Vol. 26). Curran Associates, Inc. Retrieved 2022-02-23, from https://papers.nips.cc/paper/2013/hash/ 1cecc7a77928ca8133fa24680a88d2f9-Abstract.html
Brock, A., Donahue, J., & Simonyan, K. (2019, February). Large Scale GAN Training for High Fidelity Natural Image Synthesis. arXiv:1809.11096 [cs, stat]. Retrieved 2022-02-23, from http://arxiv.org/abs/1809.11096
Brown, N., & Sandholm, T. (2017). Libratus: The Superhuman AI for No-Limit Poker. Retrieved 2022-02-23, fromhttps://www.ijcai.org/proceedings/2017/772
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., . . . Amodei, D. (2020a). Language Models are Few-Shot Learners. (_eprint: 2005.14165)
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., . . . Amodei, D. (2020b, May). Language Models are Few-Shot Learners. Retrieved 2022-02-23, from https://arxiv.org/abs/2005.14165v4
Cai, H., Gan, C., Wang, T., Zhang, Z., & Han, S. (2019, August). Once-for-All: Train One Network and Specialize it for Efficient Deployment. Retrieved 2022-02-23, from https://arxiv.org/abs/1908.09791v5
Cai, H., Zhu, L., & Han, S. (2018, December). ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. Retrieved 2022-02-23, from https://arxiv.org/abs/1812.00332v2
Catanzaro, B., Sundaram, N., & Keutzer, K. (2008). Fast Support Vector Machine Training and Classification on Graphics Processors. In Proceedings of the 25th international conference on Machine learning (pp. 104–111). New York, NY, USA: Association for Computing Machinery. Retrieved from https://doi.org/10.1145/ 1390156.1390170 (event-place: Helsinki, Finland) doi: 10.1145/1390156.1390170
7
Chellapilla, K., Puri, S., & Simard, P. (2006, October). High Performance Convolutional Neural Networks for Document Processing. In G. Lorette (Ed.), Tenth International Workshop on Frontiers in Handwriting Recognition. La Baule (France): Suvisoft. Retrieved from https://hal.inria.fr/inria-00112631 (Backup Publisher: Université de Rennes 1)
Chen, M., Radford, A., & Sutskever, I. (2020, June). Image GPT. Retrieved 2022-02-23, from https://openai.com/ blog/image-gpt/
Chen, S.-H., Hwang, S.-H., & Wang, Y.-R. (1998, May). An RNN-based prosodic information synthesizer for Mandarin text-to-speech. IEEE Transactions on Speech and Audio Processing, 6(3), 226–239. (Conference Name: IEEE Transactions on Speech and Audio Processing) doi: 10.1109/89.668817
Chollet, F. (2017, April). Xception: Deep Learning with Depthwise Separable Convolutions. arXiv:1610.02357 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1610.02357
Cires ̧an, D., Meier, U., Masci, J., & Schmidhuber, J. (2012). Multi-column deep neural network for traffic sign classification. Neural Networks, 32, 333–338. Retrieved from https://www.sciencedirect.com/science/ article/pii/S0893608012000524 doi: https://doi.org/10.1016/j.neunet.2012.02.023
Cires ̧an,D.,Meier,U.,&Schmidhuber,J. (2012,February). Multi-columnDeepNeuralNetworksforImage Classification. arXiv:1202.2745 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1202.2745
Cires ̧an, D. C., Giusti, A., Gambardella, L. M., & Schmidhuber, J. (2013). Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks. In K. Mori, I. Sakuma, Y. Sato, C. Barillot, & N. Navab (Eds.), Medical Image Computing and Computer-Assisted Intervention – MICCAI 2013 (pp. 411–418). Berlin, Heidelberg: Springer. doi: 10.1007/978-3-642-40763-5_51
Cires ̧an, D. C., Meier, U., Gambardella, L. M., & Schmidhuber, J. (2010a, December). Deep, Big, Simple Neural Nets for Handwritten Digit Recognition. Neural Computation, 22(12), 3207–3220. Retrieved from https://doi.org/ 10.1162/NECO_a_00052 (_eprint: https://direct.mit.edu/neco/article-pdf/22/12/3207/842857/neco_a_00052.pdf) doi: 10.1162/NECO_a_00052
Cires ̧an, D. C., Meier, U., Gambardella, L. M., & Schmidhuber, J. (2010b, December). Deep, big, simple neural nets for handwritten digit recognition. Neural Computation, 22(12), 3207–3220. doi: 10.1162/NECO_a_00052
Cires ̧an, D. C., Meier, U., Masci, J., Gambardella, L. M., & Schmidhuber, J. (2011). Flexible, High Performance Convolutional Neural Networks for Image Classification. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Two (pp. 1237–1242). AAAI Press. (event-place: Barcelona, Catalonia, Spain)
Computer Progress. (2022). Computer Progress. Retrieved from https://computerprogress.com/
Dai, J., Li, Y., He, K., & Sun, J. (2016, May). R-FCN: Object Detection via Region-based Fully Convolutional
Networks. Retrieved 2022-02-23, from https://arxiv.org/abs/1605.06409v2
Deng, L., Yu, D., & Hinton, G. E. (2009, December). Deep Learning for Speech Recognition and Related Applications.
Desislavov, R., Martínez-Plumed, F., & Hernández-Orallo, J. (2021). Compute and Energy Consumption Trends in Deep Learning Inference. (_eprint: 2109.05472)
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018, October). BERT: Pre-training of Deep Bidirectional Trans- formers for Language Understanding. Retrieved 2022-02-23, from https://arxiv.org/abs/1810.04805v2
Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., . . . Tang, J. (2021, May). CogView: Mastering Text-to- Image Generation via Transformers. Retrieved 2022-02-23, from https://arxiv.org/abs/2105.13290v3
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... Houlsby, N. (2020, September). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.. Retrieved 2022-02-23, from https://openreview.net/forum?id=YicbFdNTTy
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., . . . Kavukcuoglu, K. (2018, June). IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. arXiv:1802.01561 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1802.01561
Fedus, W., Zoph, B., & Shazeer, N. (2021, January). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Retrieved 2022-02-23, from https://arxiv.org/abs/2101.03961v1
Fukushima, K. (1980, April). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4), 193–202. Retrieved 2022-02-23, from https://doi.org/10.1007/BF00344251 doi: 10.1007/BF00344251
8
Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 249–256). JMLR Workshop and Conference Proceedings. Retrieved 2022-02-23, from https://proceedings.mlr.press/ v9/glorot10a.html (ISSN:1938-7228)
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., . . . Bengio, Y. (2014, June). Generative Adversarial Networks. arXiv:1406.2661 [cs, stat]. Retrieved 2022-02-23, from http://arxiv.org/ abs/1406.2661
Goyal, P., Caron, M., Lefaudeux, B., Xu, M., Wang, P., Pai, V., . . . Bojanowski, P. (2021, March). Self-supervised Pretraining of Visual Features in the Wild. Retrieved 2022-02-23, from https://arxiv.org/abs/2103.01988v2
GPT-Neo. (n.d.). Retrieved 2022-02-23, from https://www.eleuther.ai/projects/gpt-neo/
Graves, A., & Schmidhuber, J. (2005, July). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5), 602–610. Retrieved 2022-02-23, from https:// www.sciencedirect.com/science/article/pii/S0893608005001206 doi: 10.1016/j.neunet.2005.06.042
H, D. (2020). How Much Did AlphaGo Zero Cost. Retrieved from https://www.yuzeh.com/data/agz-cost.html Hardware- Und Nachrichten-Links Des 30./31. Oktober 2021. (2022, January). 3D Center. Retrieved from https://
 www.3dcenter.org/news/hardware-und-nachrichten-links-des-3031-oktober-2021

Hazelwood, K., Bird, S., Brooks, D., Chintala, S., Diril, U., Dzhulgakov, D., . . . Wang, X. (2018). Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA) (pp. 620–629). doi: 10.1109/HPCA.2018.00059
He, K., Zhang, X., Ren, S., & Sun, J. (2014). Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. arXiv:1406.4729 [cs], 8691, 346–361. Retrieved 2022-02-23, from http://arxiv.org/abs/ 1406.4729 doi: 10.1007/978-3-319-10578-9_23
He, K., Zhang, X., Ren, S., & Sun, J. (2015a, December). Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1512.03385
He, K., Zhang, X., Ren, S., & Sun, J. (2015b, February). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. arXiv:1502.01852 [cs]. Retrieved 2022-02-23, from http://arxiv.org/ abs/1502.01852
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., . . . Zhou, Y. (2017). Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 1–19.
Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A.-r., Jaitly, N., . . . Kingsbury, B. (2012). Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Processing Magazine, 29(6), 82–97. doi: 10.1109/MSP.2012.2205597
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012, July). Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1207.0580
Ho, J., Jain, A., & Abbeel, P. (2020, June). Denoising Diffusion Probabilistic Models. Retrieved 2022-02-23, fromhttps://arxiv.org/abs/2006.11239v2
Hochreiter, S., & Schmidhuber, J. (1997, November). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780. Retrieved 2022-02-23, from https://doi.org/10.1162/neco.1997.9.8.1735 doi: 10.1162/ neco.1997.9.8.1735
Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., & Mohamed, A. (2021, June). HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. Retrieved 2022-02-23, from https://arxiv.org/abs/2106.07447v1
Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., . . . Chen, Z. (2019). GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism. In Proceedings of the 33rd International Conference on Neural Information Processing Systems (pp. 103–112). Red Hook, NY, USA: Curran Associates Inc. (Section: 1)
J, T., Sejnowski, & Rosenberg, C. R. (n.d.). Parallel Networks that Learn to Pronounce English Text. Retrieved 2022-02-23, from https://www.complex-systems.com/abstracts/v01_i01_a10/
Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., ... Graepel, T. (2019, May). Human-level performance in first-person multiplayer games with population-based deep reinforcement learning. Science, 364(6443), 859–865. Retrieved 2022-02-23, from http://arxiv.org/abs/1807.01281 doi: 10.1126/science.aau6249
9
Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., . . . Duerig, T. (2021, February). Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. Retrieved 2022-02-23, from https:// arxiv.org/abs/2102.05918v2
Jones, A. L. (2021). Scaling Scaling Laws with Board Games. arXiv preprint arXiv:2104.03113, 1–8.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., . . . Amodei, D. (2020). Scaling Laws for
Neural Language Models. (_eprint: 2001.08361)
Kingma, D. P., & Ba, J. (2017, January). Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs].
Retrieved 2022-02-23, from http://arxiv.org/abs/1412.6980
Kingma, D. P., & Welling, M. (2014, May). Auto-Encoding Variational Bayes. arXiv:1312.6114 [cs, stat]. Retrieved
2022-02-23, from http://arxiv.org/abs/1312.6114
Klein, D. (n.d.). Mighty mouse. Retrieved 2022-02-22, from https://www.technologyreview.com/2018/12/19/
 138508/mighty-mouse/

Kohs, G., Antonoglou, I., Baker, L., & Bostrom, N. (2017). AlphaGo. Moxie Pictures, Reel As Dirt.
Komatsuzaki, A. (2021, June). GPT-J-6B: 6B JAX-Based Transformer. Retrieved 2022-02-23, from https://
 arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012a). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, & K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems (Vol. 25). Curran Associates, Inc. Retrieved from https://proceedings.neurips.cc/ paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012b). ImageNet Classification with Deep Convolutional Neural Net- works. In Advances in Neural Information Processing Systems (Vol. 25). Curran Associates, Inc. Retrieved 2022-02- 23, from https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract .html
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2017, May). ImageNet Classification with Deep Convolutional Neural Networks. Commun. ACM, 60(6), 84–90. Retrieved from https://doi.org/10.1145/3065386 (Place: New York, NY, USA Publisher: Association for Computing Machinery) doi: 10.1145/3065386
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019, September). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. Retrieved 2022-02-23, from https://arxiv.org/abs/ 1909.11942v6
Leahy, C. (2022, February). Announcing GPT-NeoX-20B. Retrieved 2022-02-23, from https://blog.eleuther.ai/ announcing-20b/
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989, December). Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 1(4), 541–551. (Conference Name: Neural Computation) doi: 10.1162/neco.1989.1.4.541
Lecun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998, November). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. (Conference Name: Proceedings of the IEEE) doi: 10.1109/5.726791
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., . . . Chen, Z. (2020a). Gshard: Scaling giant models with conditional computation and automatic sharding.
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., . . . Chen, Z. (2020b, June). GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. Retrieved 2022-02-23, from https://arxiv.org/ abs/2006.16668v1
Li, C. (2020, June). OpenAI’s GPT-3 Language Model: A Technical Overview. Retrieved from https://lambdalabs .com/blog/demystifying-gpt-3/
Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., & Gonzalez, J. E. (2020). Train large, then compress: Rethinking model size for efficient training and inference of transformers. arXiv preprint arXiv:2002.11794, 1–14.
Lieber, O., Sharir, O., Lenz, B., & Shoham, Y. (n.d.). Announcing AI21 Studio and Jurassic-1 Language Models. Retrieved 2022-02-23, from https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/ 61138924626a6981ee09caf6_jurassic_tech_paper.pdf
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., . . . Wierstra, D. (2019, July). Continuous control with deep reinforcement learning. arXiv:1509.02971 [cs, stat]. Retrieved 2022-02-23, from http://arxiv.org/ abs/1509.02971
10
Lin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., . . . Yang, H. (2021, October). M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. Retrieved 2022-02-23, from https://arxiv.org/ abs/2110.03888v3
Liu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li, L.-J., . . . Murphy, K. (2017, December). Progressive Neural Architecture Search. Retrieved 2022-02-23, from https://arxiv.org/abs/1712.00559v3
Lohn, A., & Musser, M. (2022, January). How Much Longer Can Computing Power Drive Artificial Intelligence Progress? (Tech. Rep.). Center for Security and Technology. (Published: Center for Security and Technology report https://cset.georgetown.edu/publication/ai-and-compute/)
Loshchilov, I., & Hutter, F. (2017, November). Decoupled Weight Decay Regularization. Retrieved 2022-02-23, fromhttps://arxiv.org/abs/1711.05101v3
Lyzhov, A. (2021, April). Review of AI and Compute Trend Isn’t Predictive of What Is Happening. (Published: Alignment Forum (blog))
Madani, A., McCann, B., Naik, N., Keskar, N. S., Anand, N., Eguchi, R. R., . . . Socher, R. (2020, March). ProGen: Language Modeling for Protein Generation (Tech. Rep.). bioRxiv. Retrieved 2022-02-23, from https://www .biorxiv.org/content/10.1101/2020.03.07.982272v2 (Section: New Results Type: article) doi: 10.1101/ 2020.03.07.982272
Mikolov, T., Karafiát, M., Burget, L., Cernocký, J. H., & Khudanpur, S. (2010). Recurrent neural network based language model. In INTERSPEECH.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. In NIPS.
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013, December). Playing Atari with Deep Reinforcement Learning. arXiv:1312.5602 [cs]. Retrieved 2022-02-23, from http:// arxiv.org/abs/1312.5602
Moore, G. (1965). Review of \emphThe Future of Integrated Electronics. (Published: Electronics Magazine)
Moravcˇík, M., Schmid, M., Burch, N., Lisý, V., Morrill, D., Bard, N., ... Bowling, M. (2017, May). DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker. Science, 356(6337), 508–513. Retrieved 2022-02-23, from http://arxiv.org/abs/1701.01724 doi: 10.1126/science.aam6960
Mudigere, D., Hao, Y., Huang, J., Jia, Z., Tulloch, A., Sridharan, S., . . . Rao, V. (2021, April). Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models. Retrieved 2022-02-23, from https://arxiv.org/abs/2104.05158v5
nad Mikhail Pavlov, A. R., Goh, G., & Gray, S. (n.d.). DALL·E: Creating Images from Text. Retrieved 2022-02-23, from https://openai.com/blog/dall-e/
Naumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sundaraman, N., Park, J., . . . Smelyanskiy, M. (2019, May). Deep Learning Recommendation Model for Personalization and Recommendation Systems. Retrieved 2022-02-23, from https://arxiv.org/abs/1906.00091v1
Naver Corporation. (n.d.). Retrieved 2022-02-23, from https://www.navercorp.com/promotion/ pressReleasesView/30546
OpenAI. (2019, October). AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning. Re- trieved from https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II
-using-multi-agent-reinforcement-learning (Publication Title: Deepmind)
OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., . . . Zhang, L. (n.d.). Solving Rubik’s
Cube with a Robot Hand. Retrieved 2022-02-23, from https://openai.com/blog/solving-rubiks-cube/ OpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V., De ̨biak, P., . . . Zhang, S. (2019, December). Dota 2 with Large
Scale Deep Reinforcement Learning. Retrieved 2022-02-23, from https://arxiv.org/abs/1912.06680v1 OpenAI API Pricing. (2021). OpenAI. Retrieved from https://openai.com/api/pricing/
Orme, J. (2022). Report: Microsoft Handed OpenAI $500m in Azure Credits. (Published: Techerati)
Patel, N. (2021, August). Why the Global Chip Shortage is Making It So Hard to Buy a PS5. Retrieved from https://www.theverge.com/2021/8/31/22648372/willy-shih-chip-shortage
-tsmc-samsung-ps5-decoder-interview

Pham, H., Dai, Z., Xie, Q., Luong, M.-T., & Le, Q. V. (2020, March). Meta Pseudo Labels. Retrieved 2022-02-23, from https://arxiv.org/abs/2003.10580v4
11
Pomerleau, D. A. (1988). ALVINN: An Autonomous Land Vehicle in a Neural Network. In Advances in Neural Information Processing Systems (Vol. 1). Morgan-Kaufmann. Retrieved 2022-02-23, from https://proceedings .neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html
Radford, A. (2018, June). Improving Language Understanding with Unsupervised Learning. Retrieved 2022-02-23, from https://openai.com/blog/language-unsupervised/
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., . . . Sutskever, I. (2021, February). Learning Transferable Visual Models From Natural Language Supervision. Retrieved 2022-02-23, from https://arxiv.org/ abs/2103.00020v1
Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., & Sutskever, I. (2019, February). Better Language Models and Their Implications. Retrieved 2022-02-23, from https://openai.com/blog/better
-language-models/

Rae, J., Irving, G., & Weidinger, L. (n.d.). Language modelling at scale. Retrieved 2022-02-23, from https:// deepmind.com/blog/article/language-modelling-at-scale
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., . . . Liu, P. J. (2019, October). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Retrieved 2022-02-23, from https:// arxiv.org/abs/1910.10683v3
Raina, R., Madhavan, A., & Ng, A. Y. (2009a). Large-Scale Deep Unsupervised Learning Using Graphics Processors. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 873–880). New York, NY, USA: Association for Computing Machinery. Retrieved from https://doi.org/10.1145/1553374.1553486 (event-place: Montreal, Quebec, Canada) doi: 10.1145/1553374.1553486
Raina, R., Madhavan, A., & Ng, A. Y. (2009b, June). Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 873–880). New York, NY, USA: Association for Computing Machinery. Retrieved 2022-02-22, from https://doi.org/10.1145/ 1553374.1553486 doi: 10.1145/1553374.1553486
Real, E., Aggarwal, A., Huang, Y., & Le, Q. V. (2018, February). Regularized Evolution for Image Classifier Architecture Search. Retrieved 2022-02-23, from https://arxiv.org/abs/1802.01548v7
Redmon, J., & Farhadi, A. (2018, April). YOLOv3: An Incremental Improvement. arXiv:1804.02767 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1804.02767
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386–408. (Place: US Publisher: American Psychological Association) doi: 10.1037/h0042519
Rosenfeld, J. S., Rosenfeld, A., Belinkov, Y., & Shavit, N. (2019). A constructive prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673, 1–30.
Rosset, C. (2020, February). Turing-NLG: A 17-billion-parameter language model by Microsoft. Retrieved 2022-02- 23, from https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter
-language-model-by-microsoft/

Rowley, H., Baluja, S., & Kanade, T. (1998, January). Neural network-based face detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1), 23–38. (Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence) doi: 10.1109/34.655647
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986, October). Learning representations by back-propagating errors. Nature, 323(6088), 533–536. Retrieved 2022-02-23, from https://www.nature.com/articles/323533a0 (Number: 6088 Publisher: Nature Publishing Group) doi: 10.1038/323533a0
Samuel, A. L. (1959, July). Some Studies in Machine Learning Using the Game of Checkers. IBM Journal of Research and Development, 3(3), 210–229. (Conference Name: IBM Journal of Research and Development) doi: 10.1147/rd.33.0210
Sastry, G., Clark, J., Brockman, G., & Sutskever, I. (2019, November). Review of AI and Compute Addendum: Compute Used in Older Headline Results. (Published: OpenAI Blog)
Schuster, T., Ram, O., Barzilay, R., & Globerson, A. (2019, February). Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing. Retrieved 2022-02-23, from https:// arxiv.org/abs/1902.09492v2
Selfridge, O. G. (n.d.). Pandemonium: A Paradigm for Learning | AITopics. Retrieved 2022-02-22, from https:// aitopics.org/doc/classics:504E1BAC/
12
Senior, A. W., Evans, R., Jumper, J., Kirkpatrick, J., Sifre, L., Green, T., . . . Hassabis, D. (2020, January). Improved protein structure prediction using potentials from deep learning. Nature, 577(7792), 706–710. Retrieved 2022-02-23, from https://www.nature.com/articles/s41586-019-1923-7 (Number: 7792 Publisher: Nature Publishing Group) doi: 10.1038/s41586-019-1923-7
Sevilla, J., Heim, L., Hobbhahn, M., Besiroglu, T., & Ho, A. (2022, January). Estimating train- ing compute of Deep Learning models. Retrieved from https://docs.google.com/document/d/ 1J2BX9jkE5nN5EA1zYRN0lHhdCf1YkiFERc_nwiYqCOA
Sevilla, J., Villalobos, P., & Cerón, J. F. (2021, June). Review of \emphParameter Counts in Machine Learning. (Published: Alignment Forum (blog))
Sharir, O., Peleg, B., & Shoham, Y. (2020). The Cost of Training NLP Models: A Concise Overview. (_eprint: 2004.08900)
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017, January). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. arXiv:1701.06538 [cs, stat]. Retrieved 2022-02-23, from http://arxiv.org/abs/1701.06538
Shilov, A. (2020, December). GPU Shortages Hit Nvidia’s Data Center Business: Not Enough $15,000+ GPUs. (Published: Tom’s Hardware)
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019, September). Megatron- LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. Retrieved 2022-02-23, from https://arxiv.org/abs/1909.08053v4
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., . . . Hassabis, D. (2016, January). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484–489. Retrieved 2022- 02-23,fromhttps://www.nature.com/articles/nature16961 (Number:7587Publisher:NaturePublishing Group) doi: 10.1038/nature16961
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., . . . Hassabis, D. (2017, December). Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. arXiv:1712.01815 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1712.01815
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., . . . Hassabis, D. (2017a). Mastering the game of Go without human knowledge. Nature, 550, 354–359.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., . . . Hassabis, D. (2017b, October). Mastering the game of Go without human knowledge. Nature, 550(7676), 354–359. Retrieved 2022-02-23, from https://www.nature.com/articles/nature24270 (Number: 7676 Publisher: Nature Publishing Group) doi: 10.1038/nature24270
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., . . . Hassabis, D. (2017c, October). Mastering the game of Go without human knowledge. Nature, 550(7676), 354–359. Retrieved 2022-02-23, from https://www.nature.com/articles/nature24270 (Number: 7676 Publisher: Nature Publishing Group) doi: 10.1038/nature24270
Simonyan, K., & Zisserman, A. (2015, April). Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv:1409.1556 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1409.1556
So, D. R., Man ́ke, W., Liu, H., Dai, Z., Shazeer, N., & Le, Q. V. (2021, September). Primer: Searching for Efficient Transformers for Language Modeling. Retrieved 2022-02-23, from https://arxiv.org/abs/2109.08668v2
Steinkraus, D., Buck, I., & Simard, P. (2005). Using GPUs for machine learning algorithms. In Eighth International Conference on Document Analysis and Recognition (ICDAR’05) (pp. 1115–1120 Vol. 2). doi: 10.1109/ICDAR.2005 .251
Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017a). Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision (pp. 843–852).
Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017b, August). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. arXiv:1707.02968 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1707.02968
Sutskever, I., Vinyals, O., & Le, Q. V. (2014, December). Sequence to Sequence Learning with Neural Networks. arXiv:1409.3215 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1409.3215
Sutton, R. (2019). The bitter lesson. Incomplete Ideas (blog), 13, 12.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., . . . Rabinovich, A. (2015, June). Going deeper with convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 1–9). (ISSN: 1063-6919) doi: 10.1109/CVPR.2015.7298594
13
Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., & Le, Q. V. (2018, July). MnasNet: Platform- Aware Neural Architecture Search for Mobile. Retrieved 2022-02-23, from https://arxiv.org/abs/1807 .11626v3
Tesauro, G. (1992, May). Practical issues in temporal difference learning. Machine Learning, 8(3), 257–277. Retrieved 2022-02-23,fromhttps://doi.org/10.1007/BF00992697 doi:10.1007/BF00992697
Thompson, N. C., Greenewald, K., Lee, K., & Manso, G. F. (2020). The Computational Limits of Deep Learning. (_eprint: 2007.05558)
Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., . . . Le, Q. (2022, January). LaMDA: Language Models for Dialog Applications. Retrieved 2022-02-23, from https://arxiv.org/abs/ 2201.08239v3
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017, December). Attention Is All You Need. arXiv:1706.03762 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/ 1706.03762
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., . . . Silver, D. (2019a). Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 1–5.
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., . . . Silver, D. (2019b, November). Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782), 350–354. Retrieved 2022-02-23,fromhttps://www.nature.com/articles/s41586-019-1724-z (Number:7782Publisher:Na- ture Publishing Group) doi: 10.1038/s41586-019-1724-z
Viola, P., & Jones, M. (2001a). Rapid object detection using a boosted cascade of simple features. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001 (Vol. 1, pp. I–I). doi: 10.1109/CVPR.2001.990517
Viola, P., & Jones, M. (2001b, December). Rapid object detection using a boosted cascade of simple features. In
Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001 (Vol. 1, pp. I–I). (ISSN: 1063-6919) doi: 10.1109/CVPR.2001.990517
Wang, K. (2020, January). DeepMind achieved StarCraft II GrandMaster Level, but at what cost? Retrieved fromhttps://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what
-cost-32891dd990e4#::text=According%20to%20the%20analysis%20by,Source%3A%20DeepMind.

Wang, L., Zhao, Y., Jinnai, Y., Tian, Y., & Fonseca, R. (2019, March). AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search. Retrieved 2022-02-23, from https://arxiv.org/abs/ 1903.11059v2
Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J., & Tang, J. (2019, November). KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation. Retrieved 2022-02-23, from https:// arxiv.org/abs/1911.06136v3
Widrow, B., & Hoff, M. E. (1988, January). Adaptive switching circuits. In Neurocomputing: foundations of research (pp. 123–134). Cambridge, MA, USA: MIT Press.
Wiggers, K. (2021, January). Google Trained a Trillion-Parameter AI Language Model. (Published: VentureBeat) Woodie, A. (2021, March). The Chip Shortage Seems to Be Impacting AI Workloads in the Cloud. (Published:
Datanami)
Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., . . . Zhang, X. (2021, October). Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning. Retrieved 2022-02-23, from https://arxiv.org/abs/ 2110.04725v2
Wu, X., Zhang, C., & Du, W. (2021, July). An Analysis on the Crisis of “Chips shortage” in Automobile Industry ——Based on the Double Influence of COVID-19 and Trade Friction. Journal of Physics: Conference Series, 1971(1), 012100. Retrieved from https://doi.org/10.1088/1742-6596/1971/1/012100 (Publisher: IOP Publishing)
doi: 10.1088/1742-6596/1971/1/012100
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., . . . Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. CoRR, abs/1609.08144. Retrieved 2022-02-23, from http://arxiv.org/abs/1609.08144
Zeiler, M. D., & Fergus, R. (2013, November). Visualizing and Understanding Convolutional Networks. arXiv:1311.2901 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1311.2901
14
Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., . . . Tian, Y. (2021, April). PanGu-$\alpha$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation. Retrieved 2022-02-23, from https://arxiv.org/abs/2104.12369v1
Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2021, June). Scaling Vision Transformers. Retrieved 2022-02-23, from https://arxiv.org/abs/2106.04560v1
Zhang, Z., Han, X., Zhou, H., Ke, P., Gu, Y., Ye, D., . . . Sun, M. (2020, December). CPM: A Large-scale Generative Chinese Pre-trained Language Model. Retrieved 2022-02-23, from https://arxiv.org/abs/2012.00413v1
Zhang, Z., Zhang, H., Zhao, L., Chen, T., & Pfister, T. (2021, May). Aggregating Nested Transformers. Retrieved 2022-02-23, from https://arxiv.org/abs/2105.12723v2
Zoph, B., & Le, Q. V. (2017, February). Neural Architecture Search with Reinforcement Learning. arXiv:1611.01578 [cs]. Retrieved 2022-02-23, from http://arxiv.org/abs/1611.01578
15
A Methods
All models in our dataset are mainly chosen from papers that meet a series of necessary criteria (has an explicit learning component, showcases experimental results, and advances the state-of-the-art) and at least one notability criterion (>1000 citations, historical importance, important SotA advance, or deployed in a notable context). For new models (from 2020 onward) it is harder to assess these criteria, so we fall back to a subjective selection. We refer to models meeting our selection criteria as milestone models.
We curated this collection of systems from various sources, including literature reviews, Papers With Code8, historical accounts, previous datasets, most cited publications of top conferences, and suggestions from individuals.
This dataset is biased in a number of important ways, and it is likely to contain mistakes. Beware of jumping to strong conclusions from our data. We discuss the limitations of our investigation in Appendix H.
When the training compute is not shared in the paper, we follow the techniques in AI and Compute’s appendix to estimate the training compute of our models (Amodei & Hernandez, 2018). These include estimating the total training compute from the forward pass compute, and from GPU time. A detailed description of our guidelines to estimate training compute is available online (Sevilla et al., 2022). Our reasoning for each estimate is annotated in the respective cells of the main dataset.
ML systems are often trained multiple times to choose better hyperparameters (e.g. number of layers or training rate). However, this information is often not reported in papers. Our dataset only annotates the compute used for the final training run.
The regressions and doubling rates are derived from log-linear fits to the training compute. Where confidence intervals are indicated, those are derived from a bootstrap with B = 1000 samples. To account for the uncertainty of our estimates, we randomly adjust each estimate by randomly multiplying it by a number between 12 and 2.9 We use the notation [quantile 0.025; median; quantile 0.975] to indicate 95% confidence intervals.
Throughout the article, we have excluded low-compute outliers from the dataset. To do so, we compute the log training compute Z-score of each model with respect to other models whose publication date is within 1.5 years. We exclude models whose Z-score is 2 standard deviations below the mean.10 This criteria results in the exclusion of 5 models out of 123 between 1952 and 2022. The models excluded this way are often from relatively novel domains, such as poker, Hanabi, and hide and seek.
Later we used a similar methodology to automatically select papers with exceedingly high compute, choosing papers that exceed the Z > 0.76 threshold after 2016. In both cases, we first decided by visual inspection which papers to mark as outliers and then chose the thresholds accordingly to automatically select them.
Key term
All models Regular-scale Large-scale Outliers
Intuitive explanation
All models after filtering low compute outliers Models excluding large scale models
Models with exceedingly high compute relative to their time Models with low compute
Formal meaning
Z > −2 −2 < Z < 0.76 Z ≥ 0.76
Z < −2
Table 5: Explanation of the different selection criteria we apply through the article. The Z value represents the distance of the log compute of each system relative to the mean of systems published within 2 years of the paper in question, normalized by the standard deviation.
8 https://paperswithcode.com/
9We use a factor of 2 for the range as this matches the range of empirical differences we found when using two different methods to estimate the training compute for a few papers (Sevilla et al., 2022). The concrete distribution we sample the random adjustment from is log uniform between 12 and 2.
10By default we only filter outliers with low compute, since we are actively interested in studying high compute models that are pushing the boundaries of ML.
16
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 118
1e+24 1e+22 1e+20 1e+18 1e+16 1e+14 1e+12 1e+10
Figure 4: The low compute outliers (n = 7) are highlighted in green. The large-scale outliers (n = 20) starting in 2015 are highlighted in red.
B Analyzing record-setting models
We describe models which set a record in compute demand—therefore outcompeting all previously released models—as record-setting models.
In general, we caution against regressing on record-setting compute budgets, as these trends are likely to be dominated by outliers. They are more representative of expensive efforts to push the SotA, rather than of the trend pushing training compute upwards.
However, it is still informative. We find that our conclusions from the post are still supported by the record-setting models. We see a slow-growing era from 1957 to 2010, and a fast-growing era from 2010 to 2015. Around September 2015, we observed a discontinuity.
The biggest difference between our main results and the trend in record-setting models is found in 2015-2022. If we include AlphaGo Zero and AlphaGo Master in the dataset, the trend is noticeably slower, with a one-year doubling time. However, if we exclude them our results agree with our main analysis: a trend with a doubling time similar to 2010-2015.
1993 1995 1997 1999 2001 2003 2005 2007 2009 2011 2013 2015 2017 2019 2021
Publication date
17
        
Pre Deep Learning Era
  
Deep Learning Era
  
Large-Scale Era

Training compute (FLOPs)
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 26
1e+24 1e+22 1e+20 1e+18 1e+16 1e+14 1e+12 1e+10
1e+8 1e+6 1e+4 1e+2
1952 1960 1968
1976 1984 1992 2000 2008 2016
Publication date
Figure 5: Training compute trend in n = 26 record-setting models between 1952 and 2022. We have excluded AlphaGo Zero and AlphaGo Master from consideration.
Table 6: Trends in record setting models.
18
R2
0.83 0.81 0.66 0.93
             
Pre Deep Learning Era
Theseus
Pandemonium (morse) Samuel Neural Checkers
Perceptron Mark I
MS
Visuali KN5 LM + RNN 400/10 (WSJ
NPLM
Decision tree (classification) TD-Gammon LSTM
NetTalkALVINN
Megatron-
Megat Alpha
GNM AlphaGo Lee
AlphaGo Fan
RA (C, PReLU)
VGG16 SPPNet
TransE AzlienxgNCeNtNs
)
Deep Learning Era
Turing NLG 530B
JuGrPasTs-3ic-117-5JBumbo AlphaStar
ron-BERT Zero
T
Large-Scale Era

Training compute (FLOPs)
Period Data Scale (FLOPs) Slope
Deep Learning Era Record-setting 2009-2016 models (n = 7)
6e+16 / 3e+18
0.4 OOMs/year [0.3; 0.5; 1.2]
7.2 months [3.1; 7.0; 11.9]
Record-setting models Large-Scale Era (n = 7)
4e+21 / 1e+24
0.4 OOMs/year [0.3; 0.4; 1.6]
8.5 months [2.3; 8.3; 19.3] 7.5 months [6.1; 7.6; 7.1]
AlphaGo Master
and Zero excluded (n = 9)
1e+21 / 1e+24
0.5 OOMs/year [0.3; 0.5; 0.6]
2016-2022
Doubling time
Pre Deep Learning Era Record-setting 1952-2009 models (n = 10)
1e+05 / 3e+14
0.2 OOMs/year [0.1; 0.2; 0.3]
19.9 months [14.4; 19.7; 30.4]
C Trends in different domains
Different domains rely on different architectures and we can naively expect them to be subject to different scaling laws. Therefore, we have decided to study different domains separately. In this section, we investigate trends in vision, language, games, and other11 domains.
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 102
1e+25
arar EE 1e+24
gniela nrcS 1e+23
a- eL egr )speaL 1e+22
e PD OL 1e+21
F( et 1e+20
up
Domain
Vision Language Games
Other
                 
m oc
g ninia rT
1e+19 1e+18 1e+17 1e+16 1e+15 1e+14
   
2011 2012
2013 2014
2015 2016 2017
Publication date
2018 2019 2020
2021 2022
Figure 6: Training compute trends per domain between 2010 and 2022. The trends are similar in each domain, though note that systems designed for games are all over the place.
Period
2009-2022
Data
Vision (n = 25) Language (n = 36) Games (n = 13) Other (n = 19)
Scale (FLOPs)
6e+14 / 6e+21 1e+17 / 2e+23 4e+17 / 2e+23 1e+16 / 7e+21
Slope
0.6 OOMs/year [0.5; 0.6; 0.7] 0.7 OOMs/year [0.6; 0.8; 1.0] 0.8 OOMs/year [0.0; 0.8; 1.1] 0.5 OOMs/year [0.4; 0.5; 0.6]
Doubling time
6.0 months [5.1; 6.0; 7.4]
4.8 months [4.1; 4.9; 6.2] 4.5 months [-4.1; 4.6; 35.5] 6.8 months [5.8; 6.7; 8.2]
                  
Table 7: Compute trends from 2009 to 2022 per domain.
The trends in vision and language seem fairly consistent over time and grow following the same doubling pattern as the overall dataset.
However, we could not find a consistent trend in games. This could be either because data is quite sparse in the games domain, because different games are essentially different domains subject to different scaling laws or because the field has not been systematically pushing forward in games to the same extent as in other domains.12
Finally, the other domains seem to follow the same trend as the overall dataset when grouped together.
D When did the Deep Learning Era start?
The Deep Learning revolution is often noted to have started in 2012 with the creation of AlexNet (e.g., Alom et al. (2018)). Our guess is that this is up for reasonable debate, but we think that the year 2010 probably best fits the available evidence about when the era started. This section explains our reasoning.
11Other incorporates domains with fewer than 10 systems each in our dataset. This includes drawing, speech, driving, robotics, recommender systems, multimodal systems, and other domains.
12The games domain includes a mix of large-scale models by large corporations and more modest contributions. 19
AlexNet has some key features usually associated with Deep Learning: it is a large model, it was trained on GPUs, and outperformed traditional approaches. However, there are models before AlexNet that arguably have some or all of these features:
• Model size/depth. Models including neural networks at least as large as AlexNet have existed since the early 2000s (notably, Viola & Jones (2001b), Raina et al. (2009a), and Mikolov et al. (2010)). In addition, neural networks roughly as deep as AlexNet (in terms of the number of hidden layers) have existed since 2010 (notably, D. C. Cires ̧an et al. (2010b) and D. Cires ̧an, Meier, & Schmidhuber (2012) both implemented up to 9 hidden layers vs. AlexNet’s 8).
GPU-based training. The insight of using GPUs to train ML models has been around for at least 7 years prior to AlexNet, and the utility for large-scale ML models is spelled out as early as 2005 (Steinkraus et al., 2005). CNNs were trained on GPUs at least as early as 2006 (Chellapilla et al., 2006), as were some other models that were large at the time (such as the 100M parameter Deep Belief Network by Raina et al. (2009a) and the neural networks of D. C. Cires ̧an et al. (2011)). Other types of ML models, such as support-vector machines (SVMs) were previously also trained on GPUs (Catanzaro et al., 2008).
Performance. AlexNet significantly outperformed prior techniques in ImageNet. However, drastic improvements over previous results are not rare in the field of ML, not even amongst large ML models that predate AlexNet. D. C. Cires ̧an et al. (2010b, 2011) made substantial improvements over the previous state-of-the-art results on MNIST, whilst Mikolov et al. (2010) surpassed all competition at the time on the Wall Street Journal task, an NLP task. Similarly, D. Cires ̧an, Meier, & Schmidhuber (2012)’s deep CNNs (which again predates AlexNet) also beat all competitors who were using traditional techniques on a traffic sign recognition competition and improved on the state-of-the-art on several common image classification benchmarks.
In addition, there is evidence that somewhere between 2009 and early 2012 the field of speech recognition realized that Deep Learning would be capable of achieving major breakthroughs on standard tasks within the domain (and interestingly, this occurred before the September 2012 ImageNet competition that AlexNet won). In particular, Deng, Yu and Hinton’s 2009 workshop titled Deep Learning for Speech Recognition and Related Applications suggest that “deep architectures with efficient learning algorithms” would be needed to overcome challenges in the subfield (Deng et al., 2009). There is evidence that between 2009 and early-2012 this became the dominant view in the subfield. For example, G. Hinton et al. (2012) presents the “shared view” of which at the time were the top 4 Speech Recognition labs. Their view was broadly that Deep Learning-based models would enable major advances in the field. This further supports the view that the switch to Deep Learning-based methods in the field of ML predates AlexNet and occurred somewhere between 2009 and 2012.
Taking this into account, we think that 2010 is the starting date most consistent with the evidence. This is because (a) the use of GPUs to train large ML models was already common at the time, (b) there were at least a few Deep Neural Networks that achieve highly competitive levels of performance (notably Mikolov et al. (2010); D. C. Cires ̧an et al. (2010b, 2011)), and (c) this timeline is consistent with the adoption of Deep Learning within the field of Speech Recognition.
In this article, we have therefore opted to use 2010 as a default date for the start of the Deep Learning Era, though as noted in Section 3.1 our results do not change when we use the more common starting point of 2012.
E Comparison to Amodei & Hernandez’s analysis
Amodei & Hernandez (2018)’s analysis shows a 3.4 month doubling from 2012 to 2018. Our analysis suggests a 5.7
month doubling time from 2012 to 2022 (Table 3). In this section, we investigate this difference.
Our analysis differs in three points (number of samples, extended time period, and the identification of a distinct large-scale trend). Of these, either the time period or the separation of the large-scale models is enough to explain the difference between our results. To show this, we investigate the same period as in the Amodei & Hernandez (2018) dataset. The period starts with AlexNet in September 2012 and ends with AlphaZero in December 2018.
As discussed, our work suggests that between 2015 and 2017 a new trend emerged — the Large-Scale Era. We discuss two scenarios: (1) assuming our distinction into two trends and (2) assuming there is a single trend (similar to Amodei & Hernandez’s analysis).
20
Period
09-2012 to 12-2017
Regular-scale (n = 24) 2e+16 / 1e+20
0.8 OOMs/year [0.5; 0.8; 1.1]
4.5 months [3.2; 4.3; 7.8]
Data Scale (FLOPs)
Slope Doubling time
R2
0.48 0.48
0.95
0.36 0.46 0.68
0.60 0.69
0.66
Table 8: Trendline data over the same period as Amodei & Hernandez’s analysis, partitioned around the release of three landmark models: AlexNet, AlphaGo Fan and AlphaZero.
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 30
1e+24 1e+23 1e+22 1e+21 1e+20 1e+19 1e+18 1e+17 1e+16 1e+15
2013 2014
2015 2016 2017
Publication date
Figure 7: Visualization of our dataset with the two distinct trends in the same time period as Amodei & Hernandez’s analysis. We can interpret these results in two ways:
1. There is a single trend, which showed a 4 month doubling time between September 2012 and December 2017. Afterwards, the trend slowed down to a 5 month doubling time.
21
AlexNet to AlphaZero
All models (n = 31) 1e+16 / 1e+21
1.0 OOMs/year [0.6; 1.0; 1.3]
3.7 months [2.8; 3.7; 6.2]
AlphaGo Fan to AlphaZero 09-2015 to 12-2017
Large-scale (n = 7)
2e+17 / 3e+23
1.2 OOMs/year [1.0; 1.3; 1.8]
3.0 months [2.1; 2.9; 3.5]




All models (n = 62)
5e+19 / 1e+23
0.8 OOMs/year [0.5; 0.8; 1.1]
4.5 months [3.3; 4.4; 7.1]
AlphaZero to present 12-2017 to 02-2022
Regular-scale (n = 47)
2e+19 / 3e+22
0.9 OOMs/year [0.6; 0.9; 1.2]
4.2 months [3.1; 4.2; 6.0]
Large-scale (n = 15)
1e+22 / 6e+23
0.4 OOMs/year [0.3; 0.4; 0.7]
8.7 months [5.4; 8.7; 14.6]


AlexNet to present
All models (n = 93)
8e+16 / 7e+22
0.6 OOMs/year [0.5; 0.6; 0.7]
5.7 months [4.9; 5.7; 6.8]
09-2012 to 02-2022
Regular-scale (n = 72)
4e+16 / 2e+22
0.6 OOMs/year [0.5; 0.6; 0.7]
5.7 months [5.0; 5.7; 6.8]


AlphaGo Fan to present 12-2017 to 02-2022
Large-scale (n = 19)
4e+21 / 6e+23
0.3 OOMs/year [0.1; 0.3; 0.5]
10.7 months [7.8; 10.7; 27.2]


  
Deep Learning Era
  
Large-Scale Era

Training compute (FLOPs)
2. A new trend of large-scale models split off the main trend in late 2015. If we separate the large-scale models, we can see that the regular-scale trend had a similar doubling time before and after 2017. Amodei & Hernandez (2018)’s result is different from ours because they are mixing together the regular-scale and large-scale trends.
In the first interpretation, our result is different from Amodei & Hernandez (2018) because we are grouping together the pre-2017 and post-2017 trends into a single analysis.
In the second interpretation, our result is different because we are analyzing the trend in large-scale and regular-scale models differently.
We currently favor the second explanation. This is because (1) the large-scale trend story seems to better predict developments after 2017, while Lyzhov (2021) found that the single-trend story does not extend past 2017, and (2) we think that the models in the large-scale trend are explained by a drastic departure in funding.
F Are large-scale models a different category?
We hypothesized that some projects that use extraordinarily large amounts of compute are a different category of flagship models, e.g. AlphaGo/Zero or GPT-3. From 2016 onwards, companies were willing to spend significantly more compute—and therefore money—than previous trends would have predicted. AlphaGo Zero in 2017 (Silver, Schrittwieser, et al., 2017a) is estimated to have cost $35M (H, 2020) and AlphaStar (Vinyals et al., 2019b) following in 2019 with an estimated cost of $12M (K. Wang, 2020). GPT-3 (T. B. Brown et al., 2020a), a recent SotA NLP model, has been estimated to have cost around $4.6M to train (C. Li, 2020). We do not know the exact spending of the relevant companies and these should be treated as rough estimates.
It is notable that AlphaGo Zero and AlphaStar have both gathered significant media attention13 which might justify the extreme costs. On the other hand, GPT-3 is now monetized to potentially make up for its significant costs (OpenAI, 2021).
However, without inside knowledge, it is hard to evaluate whether these were just continuations of a trend or categorically different projects: Were the expected economic returns of some models significantly bigger? Was AlphaGo a unique project given this milestone? We are planning to investigate this in more detail in the future.
Another question: where should we draw the line for large-scale models? There is a reasonable case for including NASv3, Libratus, Megatron-LM, T5-3B, OpenAI Five, Turing NLG, iGPT-XL, GShard (dense), Switch, DALL-E, Pangu-α, ProtT5-XXL and HyperClova on either side of the division. For example, Figure 8 depicts an alternate reasonable choice of Large-Scale models.
In Table 9 we show the effects of choosing different Z-value thresholds to separate the Large-Scale models. 14 The differences are small.
13For example the documentary on AlphaGo (Kohs et al., 2017) or AlphaStar competing in public competitions (OpenAI, 2019). 14See Appendix A for a description of how the large-scale models are selected based on the Z-value threshold.
22
Training compute (FLOPs) of milestone Machine Learning systems over time
n = 83
1e+25 1e+24 1e+23 1e+22 1e+21 1e+20 1e+19 1e+18 1e+17 1e+16
2016 2017
2018 2019 2020 2021 2022
Publication date
Figure 8: Selection of Large-Scale models when we use the threshold Z = 0.54.
     
Large-Scale Era
AlphaGo Fan
AmoebaNet-A IMPALA
BERT-Large
AlphaGoLee
DeepSpeech2 ResNet-152 (ImageNet)
Rubik's cube DLRM-2021ProGen
AraGPT2-Mega NEO (DL:RM-2022)
Xception
PNASNet-5 YOLOv3
Population-based DRL GPT
ProxylessNAS
Part-of-sentence tagging model
Named Entity Recognition model R-FCN
AlphaGo Master
AlphaGo Zero
AlphaZero
BigGAN-deep 512x512
AlphaStar
GPT-3 175B
Meena
Megatron-Turing NLG 530B
Gopher Jurassic-1-JumboYuan 1.0LaMDA
Meta Pseudo Labels ALIGN
GNMT
NASv3 (CIFAR-10)
Libratus
MoE
Megatron-LM
iGPT-L
PrimeGrPT-Neo ViT-G/14
OpenAI TI7 DOTA 1v1 JFT
wave2vec 2.0 LARGE KEPLER
Transformer
ObjectNet AlphaX-1
DLRM-2020 Decoupled weight decayCreogsusl-alirnizgautaiol nalignment
OpenAI Five Megatron-BERT
T5-11B
SwitchProtT5-XXL
OpenAI Five T5-3B
GPT-2 MnasNet-AM1n+aSsNSeDtL-Aite3
GShard (600B)ViT-H/14
CLIP (ViT L/14@336px)
Turing NLG
AlphaFold
iGPT-XL GShard (dense)
CogView GPT-J-6B
ALBERT-xxlarge
OnceforAll CPM-Large
DALL-E
HyperClova PanGu-α
HuBERTM6-10T SEER
Transformer local-attention (NesT-B)
Training compute (FLOPs)
Period
Data Scale (FLOPs) Slope Doubling time
  
2016-2022
Regular-scale models z < 0.76, (n = 63)
3e+18 / 1e+22
0.6 OOMs/year 6.0 months [0.4; 0.6; 0.8] [4.6; 6.0; 8.5]
Large-scale models z > 0.76, (n = 20)
3e+21 / 6e+23
0.4 OOMs/year 10.3 months [0.2; 0.4; 0.5] [7.6; 10.4; 21.9]
Regular-scale models z < 0.6, (n = 57)
3e+18 / 9e+21
0.6 OOMs/year 6.0 months [0.4; 0.6; 0.8] [4.6; 6.1; 8.6]
Large-scale models z > 0.6, (n = 26)
3e+21 / 4e+23
0.3 OOMs/year 10.7 months [0.2; 0.3; 0.5] [7.8; 10.9; 19.5]
Regular-scale models z < 0.54, (n = 51)
3e+18 / 5e+21
0.5 OOMs/year 6.7 months [0.4; 0.6; 0.7] [4.9; 6.7; 9.9]
Large-scale models z > 0.54, (n = 32)
2e+21 / 3e+23
0.3 OOMs/year 11.6 months [0.2; 0.3; 0.4] [8.3; 11.6; 22.1]
Table 9: Trends in record setting models.
23
R2
0.46 0.63 0.48 0.57 0.45 0.49
 
G Causes of the possible slowdown of training budget in large-scale models between 2016 and 2022
As discussed in Section 3.2, the trend of increasing compute in large-scale models between 2016 and 2022 is slower (10 month doubling time) than the overall trend (6 month doubling time).
Some hypotheses that might explain the slowdown include:
The 2020-22 global chip shortage. This occurred as a result of strong demand for computer and electronic equipment to enable working from home (Attinasi et al., 2021; S. Wu et al., 2021), supply shocks caused by severe weather disruptions15 and trade frictions caused by the ongoing trade war between the US and China. The shortage has led to blockages in automotive manufacturing (Ajmera & Ramakrishnan, 2021). GPU prices have been higher than usual. For example, a survey of German firms revealed that in Germany and Austria, GPUs are selling for up to 3× the manufacturer’s suggested retail price (MSRP) in 2021 (3D Center, 2022). It has been reported that NVIDIA has been struggling to provide some of its latest and top-performing chips, such as the A100 (Shilov, 2020). There is also anecdotal evidence (Woodie, 2021) that the chip shortage is affecting AI training runs.16
Challenges with building the required High-Performance Computing (HPC) infrastructure. The hardware constraints involved in massive training runs (including memory limitations and communication bandwidths) force users to segment massive models into groups of layers, which are then trained in parallel (Hazelwood et al., 2018; Huang et al., 2019; Athlur et al., 2021). Designing and implementing algorithms that do this efficiently can be extremely hard, and often requires dedicated engineering teams.17,18 We suspect that the cultivating of relevant expertise and the designing, testing, and deploying of HPC infrastructure for training massive Deep Learning models has created challenges unique to the Large-Scale Era.
Budget caps. The monetary costs of training the most compute-intensive ML models can be relatively large. For example, Sharir et al. (2020) estimates that Google’s T5 project—which is by no means the biggest training run to date—might have cost a total of $10M in cloud computing costs. Maintaining a constant growth rate in the budgets dedicated to training runs might, therefore, be challenging at the massive-scale.19
Undisclosed large models. Most compute intense models stem from corporate AI labs which might not publish their results publicly.
H Limitations
Over the course of this project, we have identified several sources of uncertainty and potential weaknesses with the analysis. In this appendix we discuss these and how we have accounted for them, or why we believe that they do not pose a major problem to our conclusions.
Uncertainties in compute calculations
How much would the compute values change given the uncertainties of the inputs (e.g. utilization rate, FLOP/s)? We expect most of the compute estimates to be accurate within a factor of about two based on some comparisons we did between different estimation methods (Sevilla et al., 2022). To account for this we introduce some noise when bootstrapping – see Appendix A for more details.
Non-sampling errors
What if there are many incorrect calculations?
Ideally, our calculations should be easily verifiable; they are included as annotations for the cell in which the
15See for instance Patel (2021) and Barrett (2021).
16Woodie (2021) quotes an analyst saying: “A lot of GPU users are complaining that it’s hard for them to get the GPU time. [...] They put a job in a queue and it takes a while for it to ramp. Previously they would just say there are [some] GPUs and they were just sitting there. Now they don’t always have GPUs available, so it takes a while for them to get in the queue and get their jobs running.”
17For an account of some of these challenges, see Huang et al. (2019) and Athlur et al. (2021)
18A further challenge with massive training runs, given how neural networks are trained, is that mistakes can often not be corrected after the training, which means that getting the infrastructure right the first time is very important. Relatedly, T. B. Brown et al. (2020a) document a mistake in the training run which they found only after training and were therefore unable to correct: “We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t feasible to retrain the model.”
19For example, OpenAI received $500M worth of cloud computing credits from Microsoft (Orme, 2022). Assuming that this is their entire budget for computing resources, then this would set a hard cap on the scale of training runs they could run. If they intend this budget to fund many different experiments, the total available budgets might be of the same order of magnitude as the largest training runs to date.
24
compute estimate is contained. In practice, we have seen few corrections suggested since making our dataset public, and we expect this to be a significant source of error.
• Small sample size
What would be the consequences of a larger sample size (e.g., n = 1000)?
We expect our results to be different in a few subtle ways if we had a larger dataset. If we increased the number of models in our dataset, we would sample in greater proportion from less-cited papers than currently – which tend to involve lower-cost experiments with smaller compute budgets. If we uniformly increase the number of less-cited papers in our dataset across each era, this should affect just the intercept of our trend-lines, without affecting the slope, thereby leaving the doubling period unchanged.
However, it is easier to find recent less-cited papers than those from many decades ago (as we expect the latter to be less consistently archived). If this was the case, we expect that if we increased the number of models in our dataset, we would increase the number of lower-budget experiments in the recent past without a commensurate increase in the number of lower-budget experiments from the more distant past. This would cause the estimated doubling time in the Pre Deep Learning Era to be slightly longer.
More recently, the largest-scale models (often more highly cited) seem to have a longer doubling-time than all other models. If we were to increase the size of our dataset (which would involve sampling relatively more from smaller experiments), this would reduce the intercept and shorten the average doubling time over more recent eras.
• Selection bias
What happens if we modify the (fairly subjective) notability criteria? Overall, we are biased towards AI systems that are:
–  Found in academic publications: Less data is available about closed-source commercial systems. Additionally, some papers omit important information for determining training compute, such as the total training time.
–  Written in English: This should not be too large of a problem, since the vast majority of published scientific research is in English, and this is almost certainly the case for notable ML models.
–  Models that are subjectively ‘notable’: These are more likely to be models that are large and recent. The inclusion of a higher proportion of these models makes our estimate of doubling times higher.
25

